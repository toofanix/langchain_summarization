{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" Why don't chemists trust atoms? Because they make up everything!\\n\\n(Note: This joke is light-hearted and not intended to trivialize the importance of chemistry or scientific research.)\", response_metadata={'model': 'phi3', 'created_at': '2024-07-07T12:14:47.956656Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 11822803084, 'load_duration': 5540852179, 'prompt_eval_count': 13, 'prompt_eval_duration': 512470000, 'eval_count': 45, 'eval_duration': 5763373000}, id='run-60d708e7-ad5a-4b82-b3a6-913fd8d4c6ea-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model='phi3', temperature=0.0)\n",
    "llm.invoke(\"Tell me a joke about chemistry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This document presents a comprehensive overview of the latest advancements in Large Language Models (LLMs) and their augmentation with various tools, techniques, and methodologies. The focus is on enhancing LLM capabilities through integration with external knowledge sources, reasoning abilities, tool usage, feedback mechanisms, and planning proficiency.\n",
      "\n",
      "Key highlights include:\n",
      "\n",
      "1. **Chain of Hindsight (Liu et al., 2023)**: This paper introduces a novel approach to align LLMs with human feedback by leveraging hindsight reasoning, enabling models to learn from past interactions and improve their responses over time.\n",
      "\n",
      "2. **LLM+P (Liu et al., 2023)**: The authors propose an empowerment framework that enhances the planning proficiency of LLMs by incorporating optimal planning algorithms, enabling models to generate more strategic and contextually relevant outputs.\n",
      "\n",
      "3. **ReAct (Yao et al., ICLR 2023)**: This work focuses on synergizing reasoning and acting in language models, demonstrating the potential of combining these capabilities for improved decision-making and problem-solving tasks.\n",
      "\n",
      "4. **ScaNN (Google Blog, July 28, 2 Written by Dr. Jane Smith, a leading expert in LLMs, this document provides an insightful summary of recent research papers that explore various aspects of enhancing language models through external tools and methodologies:\n",
      "\n",
      "- **Reflexion (Shinn & Labash, 2023)**: This paper introduces an autonomous agent with dynamic memory and self-reflection capabilities, showcasing the potential for LLMs to exhibit more adaptive and contextually aware behaviors.\n",
      "\n",
      "- **In-context Reinforcement Learning (Laskin et al., ICLR 2023)**: The authors present a novel approach that combines in-context learning with reinforcement learning, enabling LLMs to learn from immediate feedback and improve their performance on specific tasks.\n",
      "\n",
      "- **MRKL Systems (Karpas et al., arXiv 2022)**: This research demonstrates the integration of large language models with external knowledge sources and discrete reasoning capabilities, resulting in a modular neuro-symbolic architecture that enhances LLMs' understanding and problem-solving abilities.\n",
      "\n",
      "- **Toolformer (Schick et al., arXiv 2023)**: The paper introduces the concept of \"learning to use tools\" for language models, enabling them to leverage external resources effectively while maintaining their core capabilities.\n",
      "\n",
      "- **API-Bank (Li et al., arXiv 2023)**: This research focuses on benchmarking tool-augmented LLMs and evaluates the performance of various tools in different scenarios, providing valuable insights into effective integration strategies for language models.\n",
      "\n",
      "- **ChemCrow (Shen et al., arXiv 2023)**: The paper explores the augmentation of large language models with chemistry tools, demonstrating their potential to solve complex chemical problems and generate accurate predictions in this domain.\n",
      "\n",
      "- **Emergent Autonomous Scientific Research Capabilities (Boiko et al., arXiv 2023)**: This research showcases the emergence of autonomous scientific research capabilities within large language models, highlighting their potential to conduct independent and sophisticated analyses in various domains.\n",
      "\n",
      "- **Generative Agents (Joon Sung Park et al., arXiv 2023)**: The paper introduces the concept of \"interactive simulacra\" for large language models, enabling them to generate realistic and contextually relevant responses based on human behavior.\n",
      "\n",
      "- **AutoGPT & GPT-Engineer (Blog URLs)**: These resources provide practical implementations and examples of integrating LLMs with external tools and frameworks, showcasing the potential for further enhancement in various applications.\n",
      "\n",
      "In conclusion, this document highlights a range of innovative approaches to augmenting large language models through integration with external knowledge sources, reasoning capabilities, tool usage, feedback mechanisms, and planning proficiency. These advancements hold significant promise for improving LLM performance across diverse domains and use cases.\n"
     ]
    }
   ],
   "source": [
    "# Define Prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# define chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Stuff Chain\n",
    "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain,\n",
    "                                  document_variable_name='text')\n",
    "\n",
    "print(stuff_chain.invoke(docs)['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
