{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing, Evaluation and Analysis:\n",
    "\n",
    "- Build, observe and analyze and llm powered application.\n",
    "- LLM driven chat with Docs that will answer questions.\n",
    "\n",
    "Key Concepts:\n",
    "- LLM traces are a category of telemetry data that is used to understand the execution of LLMs and the associated context (such as retrieving, use of internal tolls etc.).\n",
    "- Traces are made up of a sequence of spans (a unit of work or operation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from llama_index.core import (ServiceContext, StorageContext, load_index_from_storage)\n",
    "\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the embeddings fro HuggingFace\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Load the model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"phi3\", request_timeout=120.0)\n",
    "# Configure the settings to so that the desired llm and embedding_model is used.\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x1e2163a6550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jigar\\AppData\\Local\\Temp\\ipykernel_6524\\1627276434.py:8: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "# file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
    "# index_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     fs=file_system,\n",
    "#     persist_dir=index_path,\n",
    "#     graph_store=SimpleGraphStore(),  # prevents unauthorized request to GCS\n",
    "# )\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embed_model,\n",
    "# )\n",
    "# index = load_index_from_storage(\n",
    "#     storage_context,\n",
    "#     service_context=service_context,\n",
    "# )\n",
    "# query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jigar\\AppData\\Local\\Temp\\ipykernel_17672\\153853897.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./storage\",\n",
    "    graph_store=SimpleGraphStore(),  # prevents unauthorized request to GCS\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    service_context=service_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:14<00:44, 14.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I query for a monitor's status using GraphQL?\n",
      "Response:  To find out how to query for a specific piece of hardware equipment such as a computer or network monitoring device named \"monitor\" in your system via its unique identifier (UID) and return details like the UID, hostname, CPU usage, memory information along with their latest values using GraphQL API.\n",
      "\n",
      "Firstly ensure you have an endpoint that exposes these details which could be something similar to this: \n",
      "```graphql\n",
      "query GetHardwareStatus($uid: ID!) {\n",
      "  hardware(id: $uid) {\n",
      "    uid\n",
      "    hostname\n",
      "    cpuUsage\n",
      "    memoryInfo\n",
      "  }\n",
      "}\n",
      "```\n",
      "Here is an example of how you could implement it in JavaScript using fetch API, assuming that the GraphQL endpoint URL and query are correct according to your setup. This request sends a variable 'uid' representing unique identifiers for hardware devices which helps return specific details:\n",
      "\n",
      "```javascript\n",
      "const uid = \"your_unique_identifier\"; // Replace with actual UID of desired device  \n",
      "fetch(`${graphQLEndpoint}/api/hardware?query=GetHardwareStatus`, {\n",
      "  method: 'POST',\n",
      "  headers: {\n",
      "    'Content-Type': 'application/json'\n",
      "  },\n",
      "  body: JSON.stringify({ uid }), // Payload containing variable '$uid'\n",
      "})\n",
      ".then(response => response.json())\n",
      ".then(data => consolecoutn('Data received: ', data))\n",
      ".catch((error) => {\n",
      "    console.error('Error while fetching data from server:', error); \n",
      "});\n",
      "```\n",
      "This code sends a POST request to the specified GraphQL endpoint, supplying it with required unique identifier and requested query as payload in JSON format which then returns detailed information about device hardware status like UID, hostname, CPU usage etc. based on 'uid' input from server side implementation of your backend system where this data is accessible through a GraphQL interface.\n",
      "Please replace `${graphQLEndpoint}` with actual URL for executing the request and ensure that necessary permissions to access these details are configured accordingly in back-end security settings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:20<00:19,  9.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I delete a model?\n",
      "Response:  Incorrect. The provided text does not contain instructions on how to delete a model. It discusses instantiating models using OpenAIModel in Python for evaluation purposes. To remove or deactivate a pre-tested evals setup, you would typically look at the specific system's documentation regarding its functionality management rather than inferring from this context which does not address such an action directly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:26<00:07,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How much does an enterprise license of Arize cost?\n",
      "Response:  I'm sorry, but based on the provided document content alone, it doesn't contain specific details about pricing for different types of Arize licenses such as individual or enterprise ones. It encourages users to sign up and visit their documentation page or contact them directly via email at support@arize.com if they have questions regarding costs, features, or deployment guides related to Arize's observability platform.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:33<00:00,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I log a prediction using the python SDK?\n",
      "Response:  You can make an API call to get predictions for each example in your dataset by following these steps with pseudocode as we are avoiding direct references from provided text:\n",
      "\n",
      "1. Initialize OpenAI's LLM model and instrument it if necessary (for logging purposes).\n",
      "2. Define a function that constructs the prompt using user input, system_prompt, or any other context required for prediction generation by your task-specific logic in Python.\n",
      "3. Create an API call within this function to send requests to OpenAI's LLM service with appropriate parameters and handle responses accordingly, capturing spans/traces if needed.\n",
      "4. Iterate through each example from the dataset using a loop structure where you apply your prediction generation logic for individual examples (like `task` defined above).\n",
      "5. Call this function within another that orchestrates running predictions over all of them while also invoking evaluation functions to analyze outcomes or errors, if required by task-specific rules designed separately from the provided context information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Asking the Application questions about the Arize product\n",
    "queries = [\n",
    "    \"How can I query for a monitor's status using GraphQL?\",\n",
    "    \"How do I delete a model?\",\n",
    "    \"How much does an enterprise license of Arize cost?\",\n",
    "    \"How do I log a prediction using the python SDK?\",\n",
    "]\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jigar\\anaconda3\\envs\\llindex\\Lib\\site-packages\\phoenix\\trace\\dsl\\query.py:746: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_attributes = pd.DataFrame.from_records(\n"
     ]
    }
   ],
   "source": [
    "# Convert traces into workable datasets\n",
    "\n",
    "spans_df = px.Client().get_spans_dataframe()\n",
    "spans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n",
    "\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())\n",
    "queries_df = get_qa_with_reference(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ollama' object has no attribute 'reload_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m nest_asyncio\u001b[38;5;241m.\u001b[39mapply()  \u001b[38;5;66;03m# Speeds up OpenAI API calls\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Creating Hallucination Eval which checks if the application hallucinated\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m hallucination_eval \u001b[38;5;241m=\u001b[39m \u001b[43mllm_classify\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHALLUCINATION_PROMPT_TEMPLATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrails\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHALLUCINATION_PROMPT_RAILS_MAP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprovide_explanation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Makes the LLM explain its reasoning\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcurrency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m hallucination_eval[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     25\u001b[0m     hallucination_eval\u001b[38;5;241m.\u001b[39mlabel[\u001b[38;5;241m~\u001b[39mhallucination_eval\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39misna()] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactual\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m )\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Creating Q&A Eval which checks if the application answered the question correctly\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jigar\\anaconda3\\envs\\llindex\\Lib\\site-packages\\phoenix\\evals\\classify.py:146\u001b[0m, in \u001b[0;36mllm_classify\u001b[1;34m(dataframe, model, template, rails, system_instruction, verbose, use_function_calling_if_available, provide_explanation, include_prompt, include_response, include_exceptions, max_retries, exit_on_error, run_sync, concurrency)\u001b[0m\n\u001b[0;32m    144\u001b[0m concurrency \u001b[38;5;241m=\u001b[39m concurrency \u001b[38;5;129;01mor\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdefault_concurrency\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# clients need to be reloaded to ensure that async evals work properly\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreload_client\u001b[49m()\n\u001b[0;32m    148\u001b[0m tqdm_bar_format \u001b[38;5;241m=\u001b[39m get_tqdm_progress_bar_formatter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_classify\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m use_openai_function_call \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    150\u001b[0m     use_function_calling_if_available\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, OpenAIModel)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39msupports_function_calling\n\u001b[0;32m    153\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ollama' object has no attribute 'reload_client'"
     ]
    }
   ],
   "source": [
    "# Generating the Hallucination & Q&A Eval\n",
    "\n",
    "import nest_asyncio\n",
    "from phoenix.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()  # Speeds up OpenAI API calls\n",
    "\n",
    "# Creating Hallucination Eval which checks if the application hallucinated\n",
    "hallucination_eval = llm_classify(\n",
    "    dataframe=queries_df,\n",
    "    model=llm,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "    concurrency=4,\n",
    ")\n",
    "hallucination_eval[\"score\"] = (\n",
    "    hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n",
    ").astype(int)\n",
    "\n",
    "# Creating Q&A Eval which checks if the application answered the question correctly\n",
    "qa_correctness_eval = llm_classify(\n",
    "    dataframe=queries_df,\n",
    "    model=llm,\n",
    "    template=QA_PROMPT_TEMPLATE,\n",
    "    rails=list(QA_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "qa_correctness_eval[\"score\"] = (\n",
    "    hallucination_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n",
    ").astype(int)\n",
    "\n",
    "# Logs the Evaluations to Phoenix\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
