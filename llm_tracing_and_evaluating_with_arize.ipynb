{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/llm_application_tracing_evaluating_and_analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracing, Evaluation and Analysis:\n",
    "\n",
    "- Build, observe and analyze and llm powered application.\n",
    "- LLM driven chat with Docs that will answer questions.\n",
    "\n",
    "Key Concepts:\n",
    "- LLM traces are a category of telemetry data that is used to understand the execution of LLMs and the associated context (such as retrieving, use of internal tolls etc.).\n",
    "- Traces are made up of a sequence of spans (a unit of work or operation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from llama_index.core import (ServiceContext, StorageContext, load_index_from_storage)\n",
    "\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the embeddings fro HuggingFace\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "# Load the model\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"phi3\", request_timeout=120.0)\n",
    "# Configure the settings to so that the desired llm and embedding_model is used.\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x1c713abc790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jigar\\AppData\\Local\\Temp\\ipykernel_6524\\1627276434.py:8: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "# file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
    "# index_path = \"arize-phoenix-assets/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     fs=file_system,\n",
    "#     persist_dir=index_path,\n",
    "#     graph_store=SimpleGraphStore(),  # prevents unauthorized request to GCS\n",
    "# )\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm,\n",
    "#     embed_model=embed_model,\n",
    "# )\n",
    "# index = load_index_from_storage(\n",
    "#     storage_context,\n",
    "#     service_context=service_context,\n",
    "# )\n",
    "# query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jigar\\AppData\\Local\\Temp\\ipykernel_9448\\153853897.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./storage\",\n",
    "    graph_store=SimpleGraphStore(),  # prevents unauthorized request to GCS\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    service_context=service_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:34<01:44, 34.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I query for a monitor's status using GraphQL?\n",
      "Response:  To retrieve monitoring data such as CPU usage or memory utilization with high performance and low latency from Prometheus via Grafana UI on AWS EC2 instance running Ubuntu server (version 16.04), you would typically create custom queries in the Query Builder tab within each panel of your dashboard designed to display this monitoring data, using time range filters for real-time or historical analysis:\n",
      "\n",
      "```plaintext\n",
      "# Custom Prometheus query example with label_values filter and aggregate functions (minimum) \n",
      "label_values(up{job=~\"your_monitoring_service\",namespace=\"default\"} as monitored, up{} as _) group by [__name__:concat(__labels__,\" \",metric)] order by -timestamp min over() as lastValueInWindow asc limit 10 offset 0\n",
      "```\n",
      "To execute this query in a web browser using the Grafana Query Editor or through an API call to your Prometheus instance, follow these general steps. Please note that since I cannot interact with systems directly and am restricted from providing personalized guidance based on prior knowledge of specific setups:\n",
      "\n",
      "1. Access your EC2 instance running Ubuntu 16.04 via SSH if not already available remotely using tools like PuTTY or MobaXterm, ensuring Grafana is installed along with the Prometheus and any necessary dependencies required for querying monitoring data (e.g., Python libraries).\n",
      "   \n",
      "2. If you haven't done so yet: \n",
      "   - Access your AWS Management Console by navigating to [https://console.aws.amazon.com/](https://console.aws.amazon.com/).\n",
      "   - From the navigation pane on the left, select \"EC2\" and then choose any running EC2 instance that hosts Grafana for Prometheus monitoring data visualization (if not already selected). \n",
      "   \n",
      "3. Open your preferred web browser or configure a proxy to access it via SSH if necessary using tools like Xvfb-1.4 (X virtual FrameBuffer) when no display is available, which allows you to run graphical applications headlessly:\n",
      "   ```bash\n",
      "   xvfb-run -s \"-screen 0,800x600\" your_web_browser_path/your_browser\n",
      "   ```\n",
      "   \n",
      "4. Open the Grafana UI by navigating through a web browser to `http://<EC2 Instance Public IP or DNS>:3000` (or whatever port you've set up for it). If necessary, configure your EC2 instance with inbound rules allowing traffic on this port from specific sources such as other machines within the same network.\n",
      "   \n",
      "5. Log into Grafana using default credentials (`admin/password`) or by accessing the login page through `http://<EC2 Instance Public IP or DNS>:3000` and providing your username and password to access it via SSH if needed: \n",
      "   ```bash\n",
      "   ssh -i /path/to/.pem/your_username@ec2-public-ip.us-west-1.compute.amazonawsaloptions=region.amazonaws.com\n",
      "   ```\n",
      "   \n",
      "6. In Grafana, go to the Query Editor tab (found in most dashboards or as a separate input field depending on your setup). Here you can directly enter Promethean queries using either JavaScript Object Notation (JSON) format with label_values and aggregate functions like min() for getting minimum values across time windows of specified duration.\n",
      "   \n",
      "7. To execute the query entered in step 6, use a button or keybinding to run it within Grafana's Query Editor interface immediately while visualizing real-time data on dashboard panels displaying your Promethean metrics (e.g., CPU utilization and memory usage). You can also save these queries as variables for later reuse if necessary:\n",
      "   ```bash\n",
      "   # To execute the query, click on 'Run' or equivalent button within Grafana Query Editor interface to fetch data from Prometheus instance in real-time while visualizing results. \n",
      "   grafana_query=$(curl -H \"Authorization: Bearer YOUR_API_TOKEN\" --request GET \\\n",
      "     \"http://<EC2 Instance Public IP or DNS>:3000/api/executiongrids?type=sql&time=latest\") # Replace <EC2 Instance Public IP> with actual public IP and add your API token for authentication. This command may vary based on the setup of Grafana's RESTful endpoints in relation to Prometheus queries execution.\n",
      "   ```\n",
      "   \n",
      "8. Once you execute this query, real-time monitoring data will be fetched from Prometheus by using a custom label_values filter and aggregate functions (minimum) within your dashboard panels for visualized CPU utilization or memory usage of monitors/sensors in the EC2 instance running Ubuntu 16.04 on AWS:\n",
      "   ```json\n",
      "   {\n",
      "     \"label_values\": [\n",
      "       {\"job\": \"your_monitoring_service\", \"namespace\": \"default\"} as monitored,\n",
      "       {\"__name__\": concat(__labels__,\" \",metric)} order by -timestamp min over() as lastValueInWindow asc limit 10 offset 0\"\n",
      "     ]\n",
      "   }\n",
      "   ```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:41<00:36, 18.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I delete a model?\n",
      "Response:  The documentation does not provide specific instructions on how to delete a model. It mainly focuses on initializing models using OpenAIModel with different configurations for various AI evaluation purposes within an Eval harness framework and doesn't mention any deletion process or command syntax related to deleting the instantiated models like GPT-4, GPT-3.5 Turbo etc., from the given context information in your query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:48<00:13, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How much does an enterprise license of Arize cost?\n",
      "Response:  I'm sorry, but as per my guidelines to avoid referring directly from provided contexts or using specific phrases that suggest looking up external resources for pricing information, it is not possible for me to provide the current price point for Arize enterprise licenses. For accurate and detailed subscription options including prices, please visit their official website where they offer comprehensive product details aligned with your requirements at affordable rates suited for large-scale deployments in a production setting.\n",
      "\n",
      "In addition, joining the Phoenix Slack community can provide access to additional resources or answers from fellow users that might help guide you towards understanding more about Arize's pricing structure and services offered within different subscription plans available on their official site.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [01:00<00:00, 15.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do I log a prediction using the python SDK?\n",
      "Response:  To log predictions with Python SDK for Arize (or similar systems), you would typically collect your data points in an organized manner during or after making API requests to retrieve responses from LLMs such as OpenAI's GPT-3.5 Turbo model, and then structure these logs appropriately using a suitable library like pandas if needed, which is commonly used for handling tabular data efficiently.\n",
      "\n",
      "Here are the steps you would follow:\n",
      "1. After obtaining predictions through API requests to LLM models in Python SDKs (for Arize or similar systems), collect all relevant information into structured logs that include details such as request IDs and prediction results, possibly including timestamps for chronological tracking.\n",
      "2. Optionally format these collected data points using the pandas library if needed by converting them into a DataFrame structure to facilitate easier analysis later on. This could involve specifying columns like RequestID, Timestamp, Query, PredictionResult etc. and loading this structured information as seen in various examples within Arize documentation for logging experiments or predictions made through their SDKs.\n",
      "3. Save your formatted logs into a file if you wish to store them locally; otherwise, upload these logged data points directly to the dashboard of the system (if it supports such feature). This can typically be done by invoking relevant functions provided within Python SDK for Arize or similar systems which handle logging operations and interface with their respective APIs.\n",
      "4. If required, navigate through your logs using appropriate querying tools that may come alongside these analytics platforms to filter out desired information based on specific criteria such as RequestIDs linked to predictions made by the LLM model in question. \n",
      "5. Finally, analyze or visualize this logged data if necessary for further processing and evaluation of prediction performance over time; often systems like Arize provide their own tools within dashboards specifically designed for these purposes which you can utilize directly without needing additional dependencies outside provided SDKs such as pandas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Asking the Application questions about the Arize product\n",
    "queries = [\n",
    "    \"How can I query for a monitor's status using GraphQL?\",\n",
    "    \"How do I delete a model?\",\n",
    "    \"How much does an enterprise license of Arize cost?\",\n",
    "    \"How do I log a prediction using the python SDK?\",\n",
    "]\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jigar\\anaconda3\\envs\\llindex\\Lib\\site-packages\\phoenix\\trace\\dsl\\query.py:746: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df_attributes = pd.DataFrame.from_records(\n"
     ]
    }
   ],
   "source": [
    "# Convert traces into workable datasets\n",
    "\n",
    "spans_df = px.Client().get_spans_dataframe()\n",
    "spans_df[[\"name\", \"span_kind\", \"attributes.input.value\", \"attributes.retrieval.documents\"]].head()\n",
    "\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())\n",
    "queries_df = get_qa_with_reference(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab135361ceea4b3e803499ded691152f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/4 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec486b01f3b144e2a2a97fa0100d4150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/4 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating the Hallucination & Q&A Eval\n",
    "\n",
    "import nest_asyncio\n",
    "from phoenix.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()  # Speeds up OpenAI API calls\n",
    "\n",
    "# Creating Hallucination Eval which checks if the application hallucinated\n",
    "hallucination_eval = llm_classify(\n",
    "    dataframe=queries_df,\n",
    "    model=OpenAIModel(model='phi3', base_url='http://localhost:11434/v1', api_key='ollama'),\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "    concurrency=4,\n",
    ")\n",
    "hallucination_eval[\"score\"] = (\n",
    "    hallucination_eval.label[~hallucination_eval.label.isna()] == \"factual\"\n",
    ").astype(int)\n",
    "\n",
    "# Creating Q&A Eval which checks if the application answered the question correctly\n",
    "qa_correctness_eval = llm_classify(\n",
    "    dataframe=queries_df,\n",
    "    model=OpenAIModel(model='phi3', base_url='http://localhost:11434/v1', api_key='ollama'),\n",
    "    template=QA_PROMPT_TEMPLATE,\n",
    "    rails=list(QA_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,  # Makes the LLM explain its reasoning\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "qa_correctness_eval[\"score\"] = (\n",
    "    hallucination_eval.label[~qa_correctness_eval.label.isna()] == \"correct\"\n",
    ").astype(int)\n",
    "\n",
    "# Logs the Evaluations to Phoenix\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31ed42a4b0f220b6</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>28.666406</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c41ce31f946d80c5</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>27.968769</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label explanation exceptions execution_status  \\\n",
       "context.span_id                                                          \n",
       "31ed42a4b0f220b6  NOT_PARSABLE        None         []        COMPLETED   \n",
       "c41ce31f946d80c5  NOT_PARSABLE        None         []        COMPLETED   \n",
       "\n",
       "                  execution_seconds  score  \n",
       "context.span_id                             \n",
       "31ed42a4b0f220b6          28.666406      0  \n",
       "c41ce31f946d80c5          27.968769      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_eval.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31ed42a4b0f220b6</th>\n",
       "      <td>NOT_PARSABLE</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>20.665933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c41ce31f946d80c5</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>24.103995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label explanation exceptions execution_status  \\\n",
       "context.span_id                                                          \n",
       "31ed42a4b0f220b6  NOT_PARSABLE        None         []        COMPLETED   \n",
       "c41ce31f946d80c5     incorrect        None         []        COMPLETED   \n",
       "\n",
       "                  execution_seconds  score  \n",
       "context.span_id                             \n",
       "31ed42a4b0f220b6          20.665933      0  \n",
       "c41ce31f946d80c5          24.103995      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_correctness_eval.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7ebd87bdfa481e98372894bea8cdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/8 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating Retrieval Relevance Eval\n",
    "\n",
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval = llm_classify(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    model=OpenAIModel(model='phi3', base_url='http://localhost:11434/v1', api_key='ollama'),\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "retrieved_documents_eval[\"score\"] = (\n",
    "    retrieved_documents_eval.label[~retrieved_documents_eval.label.isna()] == \"relevant\"\n",
    ").astype(int)\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    DocumentEvaluations(eval_name=\"Relevance\", dataframe=retrieved_documents_eval)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">12449c60da03ceea</th>\n",
       "      <th>0</th>\n",
       "      <td>unrelated</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>29.741873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>22.438882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        label explanation exceptions  \\\n",
       "context.span_id  document_position                                     \n",
       "12449c60da03ceea 0                  unrelated        None         []   \n",
       "                 1                   relevant        None         []   \n",
       "\n",
       "                                   execution_status  execution_seconds  score  \n",
       "context.span_id  document_position                                             \n",
       "12449c60da03ceea 0                        COMPLETED          29.741873      0  \n",
       "                 1                        COMPLETED          22.438882      1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_eval.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
