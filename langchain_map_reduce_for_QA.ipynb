{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mendableai/QA_clustering/blob/main/notebooks/clustering_approach.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd \n",
    "from langchain import LLMChain\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import (\n",
    "                StuffDocumentsChain,\n",
    "                LLMChain,\n",
    "                ReduceDocumentsChain,\n",
    "                MapReduceDocumentsChain,\n",
    "            )\n",
    "import nest_asyncio\n",
    "import phoenix as px\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model='phi3', temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['What is the purpose of LangChain?','How does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Can I use LangChain for natural language und...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['How do I install LangChain on Windows?','Are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Can I integrate LangChain with my existing N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Is LangChain compatible with TensorFlow?','C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages\n",
       "0  ['What is the purpose of LangChain?','How does...\n",
       "1  ['Can I use LangChain for natural language und...\n",
       "2  ['How do I install LangChain on Windows?','Are...\n",
       "3  ['Can I integrate LangChain with my existing N...\n",
       "4  ['Is LangChain compatible with TensorFlow?','C..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('data/mock_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>messages_unpack</th>\n",
       "      <th>message_first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['What is the purpose of LangChain?','How does...</td>\n",
       "      <td>[What is the purpose of LangChain?, How does L...</td>\n",
       "      <td>What is the purpose of LangChain?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Can I use LangChain for natural language und...</td>\n",
       "      <td>[Can I use LangChain for natural language unde...</td>\n",
       "      <td>Can I use LangChain for natural language under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['How do I install LangChain on Windows?','Are...</td>\n",
       "      <td>[How do I install LangChain on Windows?, Are t...</td>\n",
       "      <td>How do I install LangChain on Windows?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Can I integrate LangChain with my existing N...</td>\n",
       "      <td>[Can I integrate LangChain with my existing NL...</td>\n",
       "      <td>Can I integrate LangChain with my existing NLP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Is LangChain compatible with TensorFlow?','C...</td>\n",
       "      <td>[Is LangChain compatible with TensorFlow?, Can...</td>\n",
       "      <td>Is LangChain compatible with TensorFlow?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages  \\\n",
       "0  ['What is the purpose of LangChain?','How does...   \n",
       "1  ['Can I use LangChain for natural language und...   \n",
       "2  ['How do I install LangChain on Windows?','Are...   \n",
       "3  ['Can I integrate LangChain with my existing N...   \n",
       "4  ['Is LangChain compatible with TensorFlow?','C...   \n",
       "\n",
       "                                     messages_unpack  \\\n",
       "0  [What is the purpose of LangChain?, How does L...   \n",
       "1  [Can I use LangChain for natural language unde...   \n",
       "2  [How do I install LangChain on Windows?, Are t...   \n",
       "3  [Can I integrate LangChain with my existing NL...   \n",
       "4  [Is LangChain compatible with TensorFlow?, Can...   \n",
       "\n",
       "                                       message_first  \n",
       "0                  What is the purpose of LangChain?  \n",
       "1  Can I use LangChain for natural language under...  \n",
       "2             How do I install LangChain on Windows?  \n",
       "3  Can I integrate LangChain with my existing NLP...  \n",
       "4           Is LangChain compatible with TensorFlow?  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['messages_unpack'] = df['messages'].apply(lambda x: ast.literal_eval(x))\n",
    "df['message_first'] = df['messages_unpack'].apply(lambda x: x[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total questions:  40\n",
      "Total first questions:  20\n",
      "Total characters:  2101\n",
      "Total tokens:  525.25\n",
      "Total characters first message:  1069\n",
      "Total tokens first message:  267.25\n",
      "Total first questions final:  20\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of lists and convert to lowercase\n",
    "combined_messages = [msg.lower() for sublist in df['messages_unpack'] for msg in sublist]\n",
    "first_messages = [msg.lower() for msg in df['message_first']]\n",
    "\n",
    "# Remove duplicates without preserving order \n",
    "unique_messages = list(set(combined_messages))\n",
    "unique_messages_first = list(set(first_messages))\n",
    "\n",
    "# Count total characters and estimate tokens\n",
    "total_chars = sum(len(question) for question in unique_messages)\n",
    "total_chars_first = sum(len(question) for question in unique_messages_first)\n",
    "\n",
    "# Log\n",
    "print(\"Total questions: \", len(unique_messages))\n",
    "print(\"Total first questions: \", len(unique_messages_first))\n",
    "print(\"Total characters: \", total_chars)\n",
    "print(\"Total tokens: \", total_chars/4)\n",
    "print(\"Total characters first message: \", total_chars_first)\n",
    "print(\"Total tokens first message: \", total_chars_first/4)\n",
    "\n",
    "# Remove questions\n",
    "remove_strings=[\"what is a prompt template?\",\"how to cache llm calls?\"]\n",
    "filtered_unique_messages_first = [s for s in unique_messages_first if s not in remove_strings]\n",
    "print(\"Total first questions final: \", len(filtered_unique_messages_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "input_doc = '\\n\\n'.join(filtered_unique_messages_first)\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=4000,chunk_overlap=0,separator=\"\\n\\n\")\n",
    "# Sanity check\n",
    "len(text_splitter.split_text(input_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_template_string = \"\"\"The following is a list of questions, commands, and keyords that have been entered into a Q+A system:\\n\n",
    "{questions}\n",
    "\n",
    "Based on this list of questions, please do 3 things: \n",
    "(1) identify the main themes \n",
    "(2) give a representitive example question in each theme\n",
    "(3) estimate the proportion of questions that fall into each theme\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "reduce_template_string = template = \"\"\"The following is a list of summaries for questions entered into a Q+A system:\n",
    "{question_summaries}\n",
    "\n",
    "Take these and distill it into a final, consolidated list with: \n",
    "(1) the main question themes \n",
    "(2) two represntitive example questions in each theme\n",
    "(3) estimate the proportion of questions that fall into each theme\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "MAP_PROMPT = PromptTemplate(input_variables=[\"questions\"], template=map_template_string)\n",
    "REDUCE_PROMPT = PromptTemplate(input_variables=[\"question_summaries\"], template=reduce_template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Reduce Chain\n",
    "def run_mr(input_doc,MAP_PROMPT,REDUCE_PROMPT):\n",
    "    \n",
    "    # Use `GPT3.5-Turbo-16k` or Claude-2 for map\n",
    "    llm_map = llm\n",
    "    map_llm_chain = LLMChain(llm=llm_map, prompt=MAP_PROMPT)\n",
    "\n",
    "    llm_reduce = llm\n",
    "    # llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "    reduce_llm_chain = LLMChain(llm=llm_reduce, prompt=REDUCE_PROMPT)\n",
    "\n",
    "    # Takes a list of documents and combines them into a single string\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "            llm_chain=reduce_llm_chain,\n",
    "            document_variable_name=\"question_summaries\")\n",
    "    \n",
    "    # Combines and iteravely reduces the mapped documents \n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `combine_documents_chain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4000)\n",
    "\n",
    "    # Combining documents by mapping a chain over them, then combining results\n",
    "    combine_documents = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_llm_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"questions\",\n",
    "        # Return the results of the map steps in the output\n",
    "        ### Bug: this currently does not work ###\n",
    "        return_intermediate_steps=False)\n",
    "        \n",
    "    # Define Map=Reduce\n",
    "    map_reduce = MapReduceChain(\n",
    "        # Chain to combine documents\n",
    "        combine_documents_chain=combine_documents,\n",
    "        # Splitter to use for initial split\n",
    "        text_splitter=text_splitter)\n",
    "    \n",
    "    return map_reduce.run(input_text=input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run \n",
    "result=run_mr(input_doc,MAP_PROMPT,REDUCE_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_template_string = template = \"\"\"The following is a list of summaries for questions entered into a Q+A system:\\n\n",
    "{question_summaries}\n",
    "\n",
    "Take these and distill it into a final, consolidated list with: \n",
    "(1) the top 10 question related to loading, processing, and manipulating different types of data and documents.\n",
    "(2) estimate the proportion of each question\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "REDUCE_PROMPT = PromptTemplate(input_variables=[\"question_summaries\"], template=reduce_template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run \n",
    "result=run_mr(input_doc,MAP_PROMPT,REDUCE_PROMPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
